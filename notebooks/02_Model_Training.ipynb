{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Training & Hyperparameter Tuning (Lab)\n",
                "\n",
                "This notebook serves as the **prototyping lab** for our recommendation pipeline. \n",
                "\n",
                "**Strategy**: 3-Way Split (Train / Validation / Test)\n",
                "1.  **Split Data**: Train (60%), Validation (20%), Test (20%).\n",
                "2.  **Hyperparameter Tuning**: Optimize ALS and SVD models using the **Validation** set.\n",
                "3.  **Evaluate**: Measure final performance (RMSE, Precision@K) on the **Test** set using the best parameters.\n",
                "4.  **Production**: Retrain on the FULL dataset with the optimal hyperparameters.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "60e8850f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/maxkaizo/rec_engine/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import scipy.sparse as sparse\n",
                "import implicit\n",
                "from surprise import Dataset, Reader, SVD, accuracy\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sklearn.model_selection import train_test_split\n",
                "import pickle\n",
                "import os\n",
                "\n",
                "# Ensure models directory exists\n",
                "os.makedirs(\"../models\", exist_ok=True)\n",
                "\n",
                "# Set seed for reproducibility\n",
                "SEED = 42"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9a2db60",
            "metadata": {},
            "source": [
                "## 1. Data Splitting (60/20/20)\n",
                "We need a Validation set to select the best Hyperparameters without overfitting the Test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "c2a07178",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total Ratings: 1000209\n",
                        "Train Set: 600125 (60.0%)\n",
                        "Validation Set: 200042 (20.0%)\n",
                        "Test Set: 200042 (20.0%)\n"
                    ]
                }
            ],
            "source": [
                "# Load Ratings\n",
                "RATINGS_FILE = \"../data/ml-1m/ratings.dat\"\n",
                "ratings_cols = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
                "ratings = pd.read_csv(RATINGS_FILE, sep='::', header=None, names=ratings_cols, engine='python', encoding='latin-1')\n",
                "ratings = ratings.drop(columns=['Timestamp'])\n",
                "\n",
                "# 1. Split into Train+Val (80%) and Test (20%)\n",
                "train_val_df, test_df = train_test_split(ratings, test_size=0.2, random_state=SEED, stratify=ratings['UserID'])\n",
                "\n",
                "# 2. Split Train+Val into Train (75% of 80% = 60%) and Validation (25% of 80% = 20%)\n",
                "train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=SEED, stratify=train_val_df['UserID'])\n",
                "\n",
                "print(f\"Total Ratings: {len(ratings)}\")\n",
                "print(f\"Train Set: {len(train_df)} ({len(train_df)/len(ratings):.1%})\")\n",
                "print(f\"Validation Set: {len(val_df)} ({len(val_df)/len(ratings):.1%})\")\n",
                "print(f\"Test Set: {len(test_df)} ({len(test_df)/len(ratings):.1%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7a6128bb",
            "metadata": {},
            "source": [
                "## 2. Hyperparameter Tuning\n",
                "We will tune:\n",
                "- **ALS**: `factors` (latent dimension), `regularization`.\n",
                "- **SVD**: `n_factors`, `lr_all` (learning rate)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5eb30db",
            "metadata": {},
            "source": [
                "### 2.1 Tuning ALS (Candidate Generation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "1000a81c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/maxkaizo/rec_engine/.venv/lib/python3.12/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 4 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
                        "  check_blas_config()\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ALS Matrix Sparsity: 97.32%\n",
                        "Tuning ALS...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 15/15 [00:01<00:00, 12.61it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ALS (factors=32, reg=0.05) -> Precision@10 (Val): 0.0940\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 15/15 [00:01<00:00, 12.52it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ALS (factors=32, reg=0.1) -> Precision@10 (Val): 0.0988\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 15/15 [00:02<00:00,  5.92it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ALS (factors=64, reg=0.05) -> Precision@10 (Val): 0.0531\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 15/15 [00:02<00:00,  5.79it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ALS (factors=64, reg=0.1) -> Precision@10 (Val): 0.0641\n",
                        "Best ALS Params: {'factors': 32, 'regularization': 0.1}\n"
                    ]
                }
            ],
            "source": [
                "# Prepare Sparse Matrices for Training\n",
                "all_users = ratings['UserID'].unique()\n",
                "all_movies = ratings['MovieID'].unique()\n",
                "\n",
                "ts_users = pd.Categorical(train_df['UserID'], categories=all_users)\n",
                "ts_movies = pd.Categorical(train_df['MovieID'], categories=all_movies)\n",
                "\n",
                "item_user_train = sparse.csr_matrix(\n",
                "    (train_df['Rating'].astype(float), (ts_movies.codes, ts_users.codes)),\n",
                "    shape=(len(all_movies), len(all_users))\n",
                ")\n",
                "\n",
                "# Create User x Item Matrix (for recommend function & filtering)\n",
                "user_item_train = item_user_train.T.tocsr()\n",
                "\n",
                "print(f\"ALS Matrix Sparsity: {100 * (1 - item_user_train.nnz / (item_user_train.shape[0] * item_user_train.shape[1])):.2f}%\")\n",
                "\n",
                "# For Validation Custom Metric (Precision@K Proxy): \n",
                "# We check if the model assigns high scores to items in the Validation Set.\n",
                "\n",
                "def evaluate_als(model, val_df, k=10, sample_size=500):\n",
                "    # Quick P@K on Validation\n",
                "    user_ids = val_df['UserID'].unique()\n",
                "    sample_users = np.random.choice(user_ids, size=min(sample_size, len(user_ids)), replace=False)\n",
                "    \n",
                "    hits = 0\n",
                "    count = 0\n",
                "    \n",
                "    # User Map Check\n",
                "    real_user_id_to_idx = {uid: i for i, uid in enumerate(all_users)}\n",
                "    idx_to_real_movie_id = {i: mid for i, mid in enumerate(all_movies)}\n",
                "\n",
                "    for uid in sample_users:\n",
                "        if uid not in real_user_id_to_idx: continue\n",
                "        u_idx = real_user_id_to_idx[uid]\n",
                "        \n",
                "        # Ground Truth from Val\n",
                "        pos_items = set(val_df[(val_df['UserID'] == uid) & (val_df['Rating'] >= 4)]['MovieID'])\n",
                "        if not pos_items: continue\n",
                "        \n",
                "        # Recommend\n",
                "        # Note: implicit requires the user_items matrix (User X Item) for filtering\n",
                "        ids, _ = model.recommend(u_idx, user_item_train[u_idx], N=k, filter_already_liked_items=False)\n",
                "        recs = {idx_to_real_movie_id[i] for i in ids}\n",
                "        \n",
                "        if len(recs & pos_items) > 0:\n",
                "            hits += len(recs & pos_items) / k\n",
                "        count += 1\n",
                "        \n",
                "    return hits / count if count > 0 else 0\n",
                "\n",
                "# --- Grid Search Loop ---\n",
                "param_grid_als = {\n",
                "    'factors': [32, 64],\n",
                "    'regularization': [0.05, 0.1]\n",
                "}\n",
                "\n",
                "best_als_score = -1\n",
                "best_als_params = None\n",
                "best_als_model = None\n",
                "\n",
                "print(\"Tuning ALS...\")\n",
                "for f in param_grid_als['factors']:\n",
                "    for r in param_grid_als['regularization']:\n",
                "        model = implicit.als.AlternatingLeastSquares(factors=f, regularization=r, iterations=15, random_state=SEED)\n",
                "        \n",
                "        # --- CORRRECCIÓN AQUÍ ---\n",
                "        # Usamos user_item_train (User x Item) en lugar de item_user_train\n",
                "        model.fit(user_item_train)\n",
                "        # ------------------------\n",
                "        \n",
                "        score = evaluate_als(model, val_df)\n",
                "        print(f\"ALS (factors={f}, reg={r}) -> Precision@10 (Val): {score:.4f}\")\n",
                "        \n",
                "        if score > best_als_score:\n",
                "            best_als_score = score\n",
                "            best_als_params = {'factors': f, 'regularization': r}\n",
                "            best_als_model = model\n",
                "\n",
                "print(f\"Best ALS Params: {best_als_params}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5ba2f11f",
            "metadata": {},
            "source": [
                "### 2.2 Tuning SVD (Ranking)\n",
                "Optimizing RMSE on Validation Set. SVD is purely for explicit rating prediction here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "832b37c3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Tuning SVD...\n",
                        "SVD (n_factors=50, lr=0.005) -> RMSE (Val): 0.8869\n",
                        "SVD (n_factors=50, lr=0.01) -> RMSE (Val): 0.9068\n",
                        "SVD (n_factors=100, lr=0.005) -> RMSE (Val): 0.8921\n",
                        "SVD (n_factors=100, lr=0.01) -> RMSE (Val): 0.9167\n",
                        "Best SVD Params: {'n_factors': 50, 'lr_all': 0.005}\n"
                    ]
                }
            ],
            "source": [
                "# Prepare Data for Surprise\n",
                "reader = Reader(rating_scale=(1, 5))\n",
                "data_train = Dataset.load_from_df(train_df[['UserID', 'MovieID', 'Rating']], reader)\n",
                "trainset = data_train.build_full_trainset()\n",
                "\n",
                "# Validation List for Accuracy Check\n",
                "val_set = list(val_df[['UserID', 'MovieID', 'Rating']].itertuples(index=False, name=None))\n",
                "\n",
                "# --- Grid Search Loop ---\n",
                "param_grid_svd = {\n",
                "    'n_factors': [50, 100],\n",
                "    'lr_all': [0.005, 0.01]\n",
                "}\n",
                "\n",
                "best_svd_rmse = float('inf')\n",
                "best_svd_params = None\n",
                "best_svd_model = None\n",
                "\n",
                "print(\"\\nTuning SVD...\")\n",
                "for nf in param_grid_svd['n_factors']:\n",
                "    for lr in param_grid_svd['lr_all']:\n",
                "        model = SVD(n_factors=nf, lr_all=lr, n_epochs=20, random_state=SEED)\n",
                "        model.fit(trainset)\n",
                "        predictions = model.test(val_set)\n",
                "        rmse = accuracy.rmse(predictions, verbose=False)\n",
                "        print(f\"SVD (n_factors={nf}, lr={lr}) -> RMSE (Val): {rmse:.4f}\")\n",
                "        \n",
                "        if rmse < best_svd_rmse:\n",
                "            best_svd_rmse = rmse\n",
                "            best_svd_params = {'n_factors': nf, 'lr_all': lr}\n",
                "            best_svd_model = model\n",
                "\n",
                "print(f\"Best SVD Params: {best_svd_params}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7cc92600",
            "metadata": {},
            "source": [
                "### 2.3 Prepare Content-Based (No Tuning needed for now)\n",
                "We simply fit TF-IDF on all movies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "c334be9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "MOVIES_FILE = \"../data/ml-1m/movies.dat\"\n",
                "movies_cols = ['MovieID', 'Title', 'Genres']\n",
                "movies = pd.read_csv(MOVIES_FILE, sep='::', header=None, names=movies_cols, engine='python', encoding='latin-1')\n",
                "\n",
                "movies['genres_str'] = movies['Genres'].str.replace('|', ' ', regex=False)\n",
                "tfidf = TfidfVectorizer(token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
                "tfidf_matrix = tfidf.fit_transform(movies['genres_str'])\n",
                "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "84479098",
            "metadata": {},
            "source": [
                "## 3. Final Test Evaluation (Prototype Pipeline)\n",
                "Now we evaluate the **Hybrid Strategy** using the BEST models on the **Test Set** (which was unseen during tuning)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "0dfa4ba5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Final Test Precision@10: 0.0864\n"
                    ]
                }
            ],
            "source": [
                "# Helper maps\n",
                "real_user_id_to_idx = {uid: i for i, uid in enumerate(all_users)}\n",
                "idx_to_real_movie_id = {i: mid for i, mid in enumerate(all_movies)}\n",
                "movie_id_to_df_idx = pd.Series(movies.index, index=movies['MovieID']).to_dict()\n",
                "\n",
                "def get_hybrid_recommendations(user_id, k=10):\n",
                "    # 1. ALS Candidates (Best Model)\n",
                "    if user_id in real_user_id_to_idx:\n",
                "        user_idx = real_user_id_to_idx[user_id]\n",
                "        ids, scores = best_als_model.recommend(user_idx, user_item_train[user_idx], N=50, filter_already_liked_items=False)\n",
                "        als_candidates = [idx_to_real_movie_id[i] for i in ids]\n",
                "    else:\n",
                "        als_candidates = []\n",
                "        \n",
                "    # 2. Content Candidates\n",
                "    user_history = train_df[train_df['UserID'] == user_id]\n",
                "    # Note: In a real scenario we'd use all history, here we use what's in Training Split\n",
                "    top_movies = user_history[user_history['Rating'] >= 4].sort_values('Rating', ascending=False).head(3)['MovieID'].tolist()\n",
                "    \n",
                "    content_candidates = []\n",
                "    for mid in top_movies:\n",
                "        if mid in movie_id_to_df_idx:\n",
                "            idx = movie_id_to_df_idx[mid]\n",
                "            sim_scores = list(enumerate(cosine_sim[idx]))\n",
                "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
                "            content_candidates.extend([movies.iloc[i[0]]['MovieID'] for i in sim_scores])\n",
                "            \n",
                "    # Union\n",
                "    all_candidates = list(set(als_candidates + content_candidates))\n",
                "    \n",
                "    # 3. SVD Ranking (Best Model)\n",
                "    scored_candidates = []\n",
                "    for mid in all_candidates:\n",
                "        est = best_svd_model.predict(user_id, mid).est\n",
                "        scored_candidates.append((mid, est))\n",
                "        \n",
                "    scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
                "    return [x[0] for x in scored_candidates[:k]]\n",
                "\n",
                "# Evaluate on Test Set\n",
                "def precision_at_k(truth_df, k=10, sample_size=200):\n",
                "    hits = 0\n",
                "    total = 0\n",
                "    sample_users = np.random.choice(truth_df['UserID'].unique(), size=sample_size)\n",
                "    \n",
                "    for uid in sample_users:\n",
                "        true_pos = set(truth_df[(truth_df['UserID'] == uid) & (truth_df['Rating'] >= 4)]['MovieID'])\n",
                "        if len(true_pos) == 0: continue\n",
                "            \n",
                "        recs = get_hybrid_recommendations(uid, k)\n",
                "        hits += len(set(recs) & true_pos) / k\n",
                "        total += 1\n",
                "    return hits / total\n",
                "\n",
                "print(f\"Final Test Precision@10: {precision_at_k(test_df):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Retrain on Full Dataset (Production)\n",
                "Using the best hyperparameters found."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "4c919d67",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retraining Full Models with: ALS(f=32, r=0.1), SVD(nf=50, lr=0.005)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 20/20 [00:02<00:00,  7.47it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "All production models saved to ../models/\n"
                    ]
                }
            ],
            "source": [
                "best_als_f = best_als_params['factors']\n",
                "best_als_r = best_als_params['regularization']\n",
                "best_svd_nf = best_svd_params['n_factors']\n",
                "best_svd_lr = best_svd_params['lr_all']\n",
                "\n",
                "print(f\"Retraining Full Models with: ALS(f={best_als_f}, r={best_als_r}), SVD(nf={best_svd_nf}, lr={best_svd_lr})\")\n",
                "\n",
                "# --- Full Data Prep ---\n",
                "full_users = pd.Categorical(ratings['UserID'], categories=all_users)\n",
                "full_movies = pd.Categorical(ratings['MovieID'], categories=all_movies)\n",
                "\n",
                "# Matriz base (Item x User)\n",
                "item_user_full = sparse.csr_matrix(\n",
                "    (ratings['Rating'].astype(float), (full_movies.codes, full_users.codes)),\n",
                "    shape=(len(all_movies), len(all_users))\n",
                ")\n",
                "\n",
                "# --- CORRECCIÓN IMPORTANTE ---\n",
                "# Transponemos a User x Item explícitamente para implicit\n",
                "user_item_full = item_user_full.T.tocsr()\n",
                "# -----------------------------\n",
                "\n",
                "data_full = Dataset.load_from_df(ratings[['UserID', 'MovieID', 'Rating']], reader)\n",
                "trainset_full = data_full.build_full_trainset()\n",
                "\n",
                "# --- Train ---\n",
                "# ALS\n",
                "als_final = implicit.als.AlternatingLeastSquares(factors=best_als_f, regularization=best_als_r, iterations=20, random_state=SEED)\n",
                "# Usamos la matriz transpuesta (User x Item)\n",
                "als_final.fit(user_item_full)\n",
                "\n",
                "# SVD\n",
                "svd_final = SVD(n_factors=best_svd_nf, lr_all=best_svd_lr, n_epochs=20, random_state=SEED)\n",
                "svd_final.fit(trainset_full)\n",
                "\n",
                "# --- Save Artifacts ---\n",
                "# ALS\n",
                "user_map = dict(enumerate(full_users.categories))\n",
                "movie_map = dict(enumerate(full_movies.categories))\n",
                "\n",
                "als_artifacts = {\n",
                "    \"model\": als_final,\n",
                "    \"user_item_matrix\": user_item_full, # Guardamos la User x Item\n",
                "    \"user_inv_map\": {v: k for k, v in user_map.items()},\n",
                "    \"movie_inv_map\": {v: k for k, v in movie_map.items()},\n",
                "    \"user_map\": user_map,\n",
                "    \"movie_map\": movie_map\n",
                "}\n",
                "with open(\"../models/als_artifacts.pkl\", \"wb\") as f: pickle.dump(als_artifacts, f)\n",
                "\n",
                "# SVD\n",
                "with open(\"../models/svd_model.pkl\", \"wb\") as f: pickle.dump(svd_final, f)\n",
                "\n",
                "# Content\n",
                "content_artifacts = {\n",
                "    \"tfidf_matrix\": tfidf_matrix,\n",
                "    \"tfidf_vectorizer\": tfidf,\n",
                "    \"cosine_sim_matrix\": cosine_sim,\n",
                "    \"movies_df\": movies[['MovieID', 'Title', 'Genres']]\n",
                "}\n",
                "with open(\"../models/content_artifacts.pkl\", \"wb\") as f: pickle.dump(content_artifacts, f)\n",
                "\n",
                "print(\"All production models saved to ../models/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "0b24a7aa",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Robustness Check (Train + Test -> Eval on Val) ---\n",
                        "Training on 800167 ratings (Train+Test)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 20/20 [00:02<00:00,  8.26it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Robustness Precision@10 (on Validation): 0.1082\n"
                    ]
                }
            ],
            "source": [
                "# --- 3.5 Robustness Check: Train on (Train + Test), Evaluate on Validation ---\n",
                "print(\"\\n--- Robustness Check (Train + Test -> Eval on Val) ---\")\n",
                "\n",
                "# 1. Combine Train + Test\n",
                "train_test_combined = pd.concat([train_df, test_df])\n",
                "\n",
                "# 2. Prepare Matrices\n",
                "tt_users = pd.Categorical(train_test_combined['UserID'], categories=all_users)\n",
                "tt_movies = pd.Categorical(train_test_combined['MovieID'], categories=all_movies)\n",
                "\n",
                "item_user_tt = sparse.csr_matrix(\n",
                "    (train_test_combined['Rating'].astype(float), (tt_movies.codes, tt_users.codes)),\n",
                "    shape=(len(all_movies), len(all_users))\n",
                ")\n",
                "# ¡IMPORTANTE! Transponer para implicit\n",
                "user_item_tt = item_user_tt.T.tocsr()\n",
                "\n",
                "# 3. Train ALS (Best Params)\n",
                "print(f\"Training on {len(train_test_combined)} ratings (Train+Test)...\")\n",
                "als_check = implicit.als.AlternatingLeastSquares(\n",
                "    factors=best_als_params['factors'], \n",
                "    regularization=best_als_params['regularization'], \n",
                "    iterations=20, \n",
                "    random_state=SEED\n",
                ")\n",
                "als_check.fit(user_item_tt)\n",
                "\n",
                "# 4. Evaluate on Validation Set\n",
                "score_val = evaluate_als(als_check, val_df)\n",
                "print(f\"Robustness Precision@10 (on Validation): {score_val:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7de4d996",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
